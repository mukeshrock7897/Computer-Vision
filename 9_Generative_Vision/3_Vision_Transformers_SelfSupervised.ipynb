{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 🔎 Vision Transformers (ViT) & Self-Supervised Learning\n",
    "\n",
    "## 🎯 Intent\n",
    "\n",
    "Apply **Transformer architectures** 🧠 to vision tasks and learn from unlabeled data 📂.\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Key Models\n",
    "\n",
    "* 🖼️ **ViT (Vision Transformer)** → splits image into patches, processes like words in NLP\n",
    "* 🌳 **DeiT (Data-efficient ViT)** → trains ViTs with fewer resources\n",
    "* 🌀 **Swin Transformer** → hierarchical, efficient, strong for detection/segmentation\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 Self-Supervised Learning\n",
    "\n",
    "* 🧩 **DINO / BYOL** → learn features without labels\n",
    "* 🔗 **Contrastive Learning** → pull similar images together, push apart dissimilar ones\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Quick Summary\n",
    "\n",
    "* 👉 Transformers work on images too 🔥\n",
    "* 👉 Self-supervision reduces need for huge labeled datasets\n",
    "* 👉 Foundation for modern vision-language & multimodal models 🌐\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
