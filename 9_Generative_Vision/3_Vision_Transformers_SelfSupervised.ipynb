{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ” Vision Transformers (ViT) & Self-Supervised Learning\n",
    "\n",
    "## ğŸ¯ Intent\n",
    "\n",
    "Apply **Transformer architectures** ğŸ§  to vision tasks and learn from unlabeled data ğŸ“‚.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Key Models\n",
    "\n",
    "* ğŸ–¼ï¸ **ViT (Vision Transformer)** â†’ splits image into patches, processes like words in NLP\n",
    "* ğŸŒ³ **DeiT (Data-efficient ViT)** â†’ trains ViTs with fewer resources\n",
    "* ğŸŒ€ **Swin Transformer** â†’ hierarchical, efficient, strong for detection/segmentation\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Self-Supervised Learning\n",
    "\n",
    "* ğŸ§© **DINO / BYOL** â†’ learn features without labels\n",
    "* ğŸ”— **Contrastive Learning** â†’ pull similar images together, push apart dissimilar ones\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… Quick Summary\n",
    "\n",
    "* ğŸ‘‰ Transformers work on images too ğŸ”¥\n",
    "* ğŸ‘‰ Self-supervision reduces need for huge labeled datasets\n",
    "* ğŸ‘‰ Foundation for modern vision-language & multimodal models ğŸŒ\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
